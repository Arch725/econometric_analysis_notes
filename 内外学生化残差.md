# 内外学生化残差

在多元回归分析中，寻找“异常值”一般思路是：如果预测值$\hat{y_i}$与真实值$y_i$的残差$e_i$过大/过小，我们就认为$\mathrm{X_i}$是异常值。那么怎么衡量残差$e_i$是否过大/过小呢？

一个自然的想法时通过构建统计量，获取$e_i$的分布，从而进行假设检验，当$p$值小于给定的范围时，我们认为$e_i$可能是异常的，即$\mathrm{X_i}$是异常值。

我们知道，当给定$\mathrm{X}$时，
$$
\mathrm{E}(\mathrm{e}) = \mathrm{M}\mathrm{E}(\epsilon) = \mathrm{0} \\
var(\mathrm{e}) = var(\mathrm{M}\epsilon) = \mathrm{E}(\mathrm{M} \epsilon \epsilon^{\mathrm{T}} \mathrm{M}^{\mathrm{T}}) = \sigma^2 \mathrm{M} \equiv \sigma^2(\mathrm{I}_{N} - \mathrm{H})
$$
可知
$$
var(e_i) = \sigma^2(1 - \mathrm{H}_{ii}) \\
\frac{e_i}{\sqrt{\sigma^2(1 - \mathrm{H}_{ii})}} \sim N(0, 1)
$$
由于我们不知道总体方差$\sigma^2$，我们需要想一个方式将$\sigma^2$消去。考虑到
$$
\frac{\mathrm{e}^{\mathrm{T}}\mathrm{e}}{\sigma^2} = \left(\frac{\epsilon}{\sigma}\right)^{\mathrm{T}}\mathrm{M}\left(\frac{\epsilon}{\sigma}\right) \sim \chi^2(N-K)
$$
我们就可以构造$t$统计量：
$$
r_i \equiv \frac{e_i}{\sqrt{\sigma^2(1 - \mathrm{H}_{ii})}} / \sqrt{\frac{\mathrm{e}^{\mathrm{T}}\mathrm{e}}{\sigma^2}/(N-K)} = \frac{e_i}{\sqrt{\frac{\mathrm{e}^{\mathrm{T}}\mathrm{e}}{N-K}(1 - \mathrm{H}_{ii})}} \stackrel{a}{\sim} t(N-K)
$$
该渐进$t$统计量$r_i$被称为学生化内残差。

注意$r_i$并不严格地服从$t$分布，因为$\frac{e_i}{\sqrt{\sigma^2(1 - \mathrm{H}_{ii})}}$和$\frac{\mathrm{e}^{\mathrm{T}}\mathrm{e}}{\sigma^2}$不是独立的（$e_i$包含在$\mathrm{e}$内）。$r_i$的精确分布是（见陈希孺《近代回归分析》P100）：
$$
\frac{r_i^2}{N-K} \sim B \left(\frac{1}{2}, \frac{N-K-1}{2} \right)
$$
为了构造更严格的统计量，我们考虑从$\mathrm{e}^{\mathrm{T}}\mathrm{e} \equiv \sum \limits_{i=1}^{N} e_i^2$中剔除$e_i$的影响。我们从数据集$\mathrm{X}, \mathrm{y}$中剔除$\mathrm{X}_i, y_i$，得到含$N-1$个观测的新数据集$\mathrm{X}_{(i)}, \mathrm{y}_{(i)}$，将$\mathrm{y}_{(i)}$对$\mathrm{X}_{(i)}$做回归，可以得到$N-1$个新残差的和：
$$
\mathrm{e}_{(i)}^{\mathrm{T}}\mathrm{e}_{(i)} = \sum \limits_{j=1, j \ne i}^{N} e_j^2
$$
同理有
$$
\frac{\mathrm{e}_{(i)}^{\mathrm{T}}\mathrm{e}_{(i)}}{\sigma^2} \sim \chi^2((N-1)-K)
$$
构造出新的$t$统计量：
$$
t_i \equiv \frac{e_i}{\sqrt{\sigma^2(1 - \mathrm{H}_{ii})}} / \sqrt{\frac{\mathrm{e}_{(i)}^{\mathrm{T}}\mathrm{e}_{(i)}}{\sigma^2}/(N-1-K)} = \frac{e_i}{\sqrt{\frac{\mathrm{e}_{(i)}^{\mathrm{T}}\mathrm{e}_{(i)}}{N-K-1}(1 - \mathrm{H}_{ii})}} \stackrel{a}{\sim} t(N-K-1)
$$
该渐进$t$统计量$t_i$被称为学生化（外）残差。遗憾的是，虽然我们简单地从分母中剔除了$e_i$，但是$t_i$仍然不是严格服从$t(N-K-1)$，当然，这已经比$r_i$的分布要更接近了，在实际应用中用$t_i$作为$t$统计量是可以的。

接下来我们从代数方面对$\mathrm{e}_{(i)}^{\mathrm{T}}\mathrm{e}_{(i)}$做一些简化。我们的目标是：尽量用第一次回归的信息去表示$\mathrm{e}_{(i)}^{\mathrm{T}}\mathrm{e}_{(i)}$，不做二次回归，从而节省计算复杂度。

由定义知
$$
\mathrm{X} = \begin{bmatrix} \mathrm{X}_{(i)} \\ \mathrm{X}_{i}^{\mathrm{T}}
\end{bmatrix} \quad \mathrm{y} = \begin{bmatrix} \mathrm{y}_{(i)} \\ y_{i}
\end{bmatrix} \\
\mathrm{e}^{\mathrm{T}}\mathrm{e} = \mathrm{y}^{\mathrm{T}}\mathrm{y} - \hat{\beta}^{\mathrm{T}}\mathrm{X}^{\mathrm{T}}\mathrm{X}\hat{\beta} \\
\mathrm{e}^{\mathrm{T}}_{(i)}\mathrm{e_{(i)}} = \mathrm{y}^{\mathrm{T}}_{(i)}\mathrm{y_{(i)}} - \hat{\beta}^{\mathrm{T}}_{(i)}\mathrm{X}^{\mathrm{T}}_{(i)}\mathrm{X}_{(i)}\hat{\beta}_{(i)} \\
\hat{\beta} = (\mathrm{X}^{\mathrm{T}}\mathrm{X})^{-1}\mathrm{X}^{\mathrm{T}}\mathrm{y} \\
\hat{\beta}_{(i)} = (\mathrm{X}^{\mathrm{T}}_{(i)}\mathrm{X}_{(i)})^{-1}\mathrm{X}^{\mathrm{T}}_{(i)}\mathrm{y}_{(i)}
$$
为了用$\mathrm{e}$表示$\mathrm{e}_{(i)}$，我们需要用$\beta$表示$\hat{\beta}_{(i)}$。由矩阵之和的求逆公式知
$$
(\mathrm{X}^{\mathrm{T}}\mathrm{X})^{-1} = \left(\begin{bmatrix} \mathrm{X}_{(i)}^{\mathrm{T}} & \mathrm{X}_{i}
\end{bmatrix} \begin{bmatrix} \mathrm{X}_{(i)} \\ \mathrm{X}_{i}^{\mathrm{T}}
\end{bmatrix} \right)^{-1} = (\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)} + \mathrm{X}_{i}\mathrm{X}_{i}^{\mathrm{T}})^{-1} \\
= (\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)})^{-1} - \frac{(\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)})^{-1}\mathrm{X}_{i}\mathrm{X}_{i}^{\mathrm{T}}(\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)})^{-1}}{1 + \mathrm{X}_{i}(\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)})^{-1}\mathrm{X}_{i}^{\mathrm{T}}}
$$
为简便起见，记矩阵$(\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)})^{-1} \equiv \mathrm{Q}^{-1}$，标量$\mathrm{X}_{i}(\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)})^{-1}\mathrm{X}_{i}^{\mathrm{T}} = \mathrm{X}_{i}\mathrm{Q}^{-1}\mathrm{X}_{i}^{\mathrm{T}}\equiv m$，则有
$$
\begin{aligned}
\hat{\beta} &= \left( \mathrm{Q}^{-1} - \frac{\mathrm{Q}^{-1}\mathrm{X}_{i}\mathrm{X}_{i}^{\mathrm{T}}\mathrm{Q}^{-1}}{1 + m} \right) \begin{bmatrix} \mathrm{X}_{(i)}^{\mathrm{T}} & \mathrm{X}_{i}
\end{bmatrix} \begin{bmatrix} \mathrm{y}_{(i)} \\ y_{i}
\end{bmatrix} \\
&= \left( \mathrm{Q}^{-1} - \frac{\mathrm{Q}^{-1}\mathrm{X}_{i}\mathrm{X}_{i}^{\mathrm{T}}\mathrm{Q}^{-1}}{1 + m} \right)(\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{y}_{(i)} + \mathrm{X}_{i}y_{i}) \\
&= \mathrm{Q}^{-1}\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{y}_{(i)} + \mathrm{Q}^{-1}\mathrm{X}_{i}y_{i} - \frac{\mathrm{Q}^{-1}\mathrm{X}_{i}\mathrm{X}_{i}^{\mathrm{T}}\mathrm{Q}^{-1}\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{y}_{(i)}}{1+m} - \frac{\mathrm{Q}^{-1}\mathrm{X}_{i}(\mathrm{X}_{i}^{\mathrm{T}}\mathrm{Q}^{-1}\mathrm{X}_{i})y_{i}}{1+m} \\
\end{aligned}
$$
注意到
$$
\mathrm{Q}^{-1}\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{y}_{(i)} = (\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)})^{-1}\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{y}_{(i)} \equiv \hat{\beta}_{(i)} \\
$$
于是
$$
\begin{aligned}
\hat{\beta} &= \hat{\beta}_{(i)} + \mathrm{Q}^{-1}\mathrm{X}_{i}y_{i} - \frac{\mathrm{Q}^{-1}\mathrm{X}_{i}\mathrm{X}_{i}^{\mathrm{T}}\hat{\beta}_{(i)}}{1+m} - \frac{m\mathrm{Q}^{-1}\mathrm{X}_{i}y_{i}}{1+m} \\
&= \hat{\beta}_{(i)} + \frac{\mathrm{Q}^{-1}\mathrm{X}_{i}y_{i}}{1+m} - \frac{\mathrm{Q}^{-1}\mathrm{X}_{i}\mathrm{X}_{i}^{\mathrm{T}}\hat{\beta}_{(i)}}{1+m} \\
&= \hat{\beta}_{(i)} + \frac{\mathrm{Q}^{-1}\mathrm{X}_{i}(y_{i} - \mathrm{X}_{i}^{\mathrm{T}}\hat{\beta}_{(i)})}{1+m} \\
\end{aligned}
$$
我们需要进一步将$y_{i} - \mathrm{X}_{i}^{\mathrm{T}}\hat{\beta}_{(i)}$用第一次回归的元素表示：
$$
\begin{aligned}
y_{i} - \mathrm{X}_{i}^{\mathrm{T}}\hat{\beta}_{(i)} &= y_{i} - \mathrm{X}_{i}^{\mathrm{T}}\hat{\beta} + \mathrm{X}_{i}^{\mathrm{T}}(\hat{\beta} - \hat{\beta}_{(i)}) \\
&= e_i + \frac{\mathrm{X}_{i}^{\mathrm{T}}\mathrm{Q}^{-1}\mathrm{X}_{i}(y_{i} - \mathrm{X}_{i}^{\mathrm{T}}\hat{\beta}_{(i)})}{1+m} \\
&= e_i + \frac{m(y_{i} - \mathrm{X}_{i}^{\mathrm{T}}\hat{\beta}_{(i)})}{1+m}
\end{aligned}
$$
直接解出$y_{i} - \mathrm{X}_{i}^{\mathrm{T}}\hat{\beta}_{(i)}$整体：
$$
y_{i} - \mathrm{X}_{i}^{\mathrm{T}}\hat{\beta}_{(i)} = e_i(1+m)
$$
代入上式得：
$$
\hat{\beta} = \hat{\beta}_{(i)} + e_i \mathrm{Q}^{-1}\mathrm{X}_{i}
$$
将$\hat{\beta}$和$\hat{\beta}_{(i)}$的关系代入$\mathrm{e}^{\mathrm{T}}\mathrm{e}$：
$$
\begin{aligned}
\mathrm{e}^{\mathrm{T}}\mathrm{e} &= \mathrm{y}^{\mathrm{T}}\mathrm{y} - \hat{\beta}^{\mathrm{T}}\mathrm{X}^{\mathrm{T}}\mathrm{X}\hat{\beta} \\
&= \begin{bmatrix} \mathrm{y}^{\mathrm{T}}_{(i)} & y_{i}
\end{bmatrix} \begin{bmatrix} \mathrm{y}_{(i)} \\ y_{i}
\end{bmatrix} - \hat{\beta}^{\mathrm{T}} \begin{bmatrix} \mathrm{X}_{(i)}^{\mathrm{T}} & \mathrm{X}_{i}
\end{bmatrix} \begin{bmatrix} \mathrm{X}_{(i)} \\ \mathrm{X}_{i}^{\mathrm{T}}
\end{bmatrix} \hat{\beta} \\
&= \mathrm{y}^{\mathrm{T}}_{(i)}\mathrm{y}_{(i)} + y_{i}^2 - \hat{\beta}^{\mathrm{T}}(\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)} + \mathrm{X}_{i}\mathrm{X}_{i}^{\mathrm{T}})\hat{\beta} \\
&= \mathrm{y}^{\mathrm{T}}_{(i)}\mathrm{y}_{(i)} + (\mathrm{X}_{i}^{\mathrm{T}}\hat{\beta} + e_i)^2 - \hat{\beta}^{\mathrm{T}}\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)}\hat{\beta}- \hat{\beta}^{\mathrm{T}}\mathrm{X}_{i}\mathrm{X}_{i}^{\mathrm{T}}\hat{\beta} \\
&= \mathrm{y}^{\mathrm{T}}_{(i)}\mathrm{y}_{(i)} + e_i^2 + 2e_i\mathrm{X}_{i}^{\mathrm{T}}\hat{\beta} - \hat{\beta}^{\mathrm{T}}\mathrm{Q}\hat{\beta} \\
\end{aligned}
$$
将$\mathrm{e}^{\mathrm{T}}_{(i)}\mathrm{e_{(i)}}$展开：
$$
\mathrm{e}^{\mathrm{T}}_{(i)}\mathrm{e_{(i)}} = \mathrm{y}^{\mathrm{T}}_{(i)}\mathrm{y_{(i)}} - \hat{\beta}^{\mathrm{T}}_{(i)}\mathrm{X}^{\mathrm{T}}_{(i)}\mathrm{X}_{(i)}\hat{\beta}_{(i)} = \mathrm{y}^{\mathrm{T}}_{(i)}\mathrm{y_{(i)}} - \hat{\beta}^{\mathrm{T}}_{(i)}\mathrm{Q}\hat{\beta}_{(i)}
$$
于是
$$
\begin{aligned}
\Delta e &\equiv \mathrm{e}^{\mathrm{T}}\mathrm{e} - \mathrm{e}^{\mathrm{T}}_{(i)}\mathrm{e_{(i)}} \\
&= e_i^2 + 2e_i\mathrm{X}_{i}^{\mathrm{T}}\hat{\beta} - \hat{\beta}^{\mathrm{T}}\mathrm{Q}\hat{\beta} + \hat{\beta}^{\mathrm{T}}_{(i)}\mathrm{Q}\hat{\beta}_{(i)} \\
&= e_i^2 + 2e_i\mathrm{X}_{i}^{\mathrm{T}}\hat{\beta}_{(i)} + 2e_i^2\mathrm{X}_{i}^{\mathrm{T}}\mathrm{Q}^{-1}\mathrm{X}_{i} - (\hat{\beta}_{(i)} + e_i \mathrm{Q}^{-1}\mathrm{X}_{i})^{\mathrm{T}}\mathrm{Q}(\hat{\beta}_{(i)} + e_i \mathrm{Q}^{-1}\mathrm{X}_{i}) + \hat{\beta}^{\mathrm{T}}_{(i)}\mathrm{Q}\hat{\beta}_{(i)} \\
&= e_i^2 + 2e_i\mathrm{X}_{i}^{\mathrm{T}}\hat{\beta}_{(i)} + 2me_i^2 - 2e_i \mathrm{X}_{i}^{\mathrm{T}}\hat{\beta}_{(i)} - e_i^2 \mathrm{X}_{i}^{\mathrm{T}}\mathrm{Q}^{-1}\mathrm{X}_{i} \\
&= (1 + m)e_i^2 \\
&\equiv (1 + \mathrm{X}_{i}(\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)})^{-1}\mathrm{X}_{i}^{\mathrm{T}})e_i^2
\end{aligned}
$$
说明从数据集中去掉一个观测之后，单准减掉该观测的残差会高估目前回归的残差（去掉一个观测之后，回归会拟合地更好，导致另外$N-1$个残差也有不同程度的下降，这也是$t_i$分子分母不独立的原因）。

由于我们希望仅由第一次回归的数据就计算出两次残差的差值，于是我们将$(\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)})^{-1}$和$(\mathrm{X}^{\mathrm{T}}\mathrm{X})^{-1}$联系起来：
$$
\begin{aligned}
\mathrm{H} &= \mathrm{X}(\mathrm{X}^{\mathrm{T}}\mathrm{X})^{-1}\mathrm{X}^{\mathrm{T}} \\
&= \begin{bmatrix} \mathrm{X}_{(i)} \\ \mathrm{X}_{i}^{\mathrm{T}}
\end{bmatrix} \left((\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)})^{-1} - \frac{(\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)})^{-1}\mathrm{X}_{i}\mathrm{X}_{i}^{\mathrm{T}}(\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)})^{-1}}{1 + \mathrm{X}_{i}(\mathrm{X}_{(i)}^{\mathrm{T}}\mathrm{X}_{(i)})^{-1}\mathrm{X}_{i}^{\mathrm{T}}}\right)^{-1} \begin{bmatrix} \mathrm{X}_{(i)}^{\mathrm{T}} & \mathrm{X}_{i}
\end{bmatrix} \\
&= \begin{bmatrix} \mathrm{X}_{(i)} \\ \mathrm{X}_{i}^{\mathrm{T}}
\end{bmatrix} \left( \mathrm{Q}^{-1} - \frac{\mathrm{Q}^{-1}\mathrm{X}_{i}\mathrm{X}_{i}^{\mathrm{T}}\mathrm{Q}^{-1}}{1 + m} \right) \begin{bmatrix} \mathrm{X}_{(i)}^{\mathrm{T}} & \mathrm{X}_{i}
\end{bmatrix}
\end{aligned}
$$
可知
$$
\begin{aligned}
\mathrm{H}_{ii} &= \mathrm{X}_{i}^{\mathrm{T}} \left( \mathrm{Q}^{-1} - \frac{\mathrm{Q}^{-1}\mathrm{X}_{i}\mathrm{X}_{i}^{\mathrm{T}}\mathrm{Q}^{-1}}{1 + m} \right) \mathrm{X}_{i} \\
&= m - \frac{\mathrm{X}_{i}^{\mathrm{T}}\mathrm{Q}^{-1}\mathrm{X}_{i}\mathrm{X}_{i}^{\mathrm{T}}\mathrm{Q}^{-1}\mathrm{X}_{i}}{1 + m} \\
&= m - \frac{m^2}{1 + m} \\
&= \frac{m}{1 + m} \\
\end{aligned}
$$
于是我们可以得到
$$
\mathrm{e}^{\mathrm{T}}_{(i)}\mathrm{e_{(i)}} = \mathrm{e}^{\mathrm{T}}\mathrm{e} - \frac{e_i^2}{1 - \mathrm{H}_{ii}}
$$
代入$t_i$的表达式：
$$
t_i \equiv \frac{e_i}{\sqrt{1 - \mathrm{H}_{ii}}} / \sqrt{\left(\mathrm{e}^{\mathrm{T}}\mathrm{e} - \frac{e_i^2}{1 - \mathrm{H}_{ii}}\right)/(N-1-K)}  \stackrel{a}{\sim} t(N-K-1)
$$
如此，我们在计算学生化（外）残差时，不需要额外的计算，只需要原有的$\mathrm{H}_{ii}$即可。


$$

$$
